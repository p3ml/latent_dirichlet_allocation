{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#0071BD;color:white;text-align:center;padding-top:0.8em;padding-bottom: 0.8em\">\n",
    "  LDA on Returns\n",
    "</h1>\n",
    "\n",
    "This notebook illustrates the core ideas of Latent Dirichlet Allocation on a very minimal corpus. After you have worked through this notebook, you should have understood:\n",
    "  * A __corpus__ consists of a list of documents.\n",
    "  * The __vocabulary__ consists of the union of words that we consider relevant in the documents.\n",
    "  * Each document is represented by the __word counts__ of the words in the vocabulary.\n",
    "  * A __topic__ is a probability distribution over the vocabulary.\n",
    "  * The __topic distribution__ gives us the share that each topic has on a given document.\n",
    "  * Topic distribution times topics is an approximation of the word counts.\n",
    "  * Very frequent and extremly seldom words do not contribute to the distinction of topics.\n",
    "  \n",
    "<p style=\"background-color:#66A5D1;padding-top:0.2em;padding-bottom: 0.2em\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comments in /this/ notebook are meant as invitations to explore alternatives.\n",
    "# On the first read, you should just ignore all the comments. On a second read\n",
    "# you might want to add more sentences to the corpus (see cells below).\n",
    "# So if this is your first read, you should start ignoring comments now.\n",
    "\n",
    "# If you enlarge the corpus, you might want to enlarge the width of the notebook\n",
    "# on the screen, to see the tables without line breaks. The two lines below make\n",
    "# the cells as wide as possible:\n",
    "\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions for printing\n",
    "Simple printing functions to present values and matrices somewhat harmoniously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cell(template, value, threshold):\n",
    "    if (threshold is None) or (value > threshold): \n",
    "        text = template.format(value)\n",
    "    else:\n",
    "        size = len(template.format(0))\n",
    "        text = size * ' '\n",
    "    print(text, end='')\n",
    "\n",
    "def print0d(value):\n",
    "    print('\\n=== {} ==='.format(value))\n",
    "\n",
    "def print1d(template, vector, threshold = None):\n",
    "    for value in vector: \n",
    "        print_cell(template, value, threshold)\n",
    "    print()\n",
    "    \n",
    "def print2d(template, matrix, threshold = None):\n",
    "    for vector in matrix: \n",
    "        print1d(template, vector, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus definition and vocabulary creation\n",
    "\n",
    "Below you find a few sentences that obviously cover two quite distinct topics. They share a common word that has two different meanings. We consider each sentence to be a separate document. Let's see whether Latend Dirichlet Allocation is able to detect that we are looking at two different topics. Notice that the process is unsupervised, i.e. **we never tell the algorithms for any document (sentence) which topic it covers**. The only hint, we will give the algorithm is that it should look out for exactly 2 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Investment in Deutsche Bank yields low return.',\n",
    "    'My investment may return nothing.', \n",
    "    'Federer’s return was good, his volley was not.',\n",
    "    'Return volley, return volley; tennis is boring.',\n",
    "    'Return on investment is on a ten year high.',\n",
    "#    'Tennis is for Federer!',\n",
    "#    'Deutsche Bank may be an investment bank.'\n",
    "]\n",
    "\n",
    "n_topics = 2\n",
    "\n",
    "expected_topics = [0, 0, 1, 1, 0] # Not for training! Just for checking afterwards.\n",
    "# expected_topics = [0, 0, 1, 1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Words per document ===\n",
      "investment  deutsche    bank        yields      return      \n",
      "investment  return      nothing     \n",
      "federer     return      good        volley      \n",
      "return      volley      return      volley      tennis      boring      \n",
      "return      investment  year        high        \n"
     ]
    }
   ],
   "source": [
    "bags = []\n",
    "\n",
    "for document in corpus:\n",
    "    \n",
    "    tokens = re.split('[ .!,;’]', document)\n",
    "    bag    = [token.lower() for token in tokens if len(token) > 3]\n",
    "    \n",
    "# Filtering words by length is just a shortcut to filtering words that do not contribute to a topic.\n",
    "# We call words that we filter out during preprocessing independently of the motivation \"stop words\". \n",
    "# Some words that hardly contribute to a topic are function words. See https://en.wikipedia.org/wiki/Function_word\n",
    "# You may instead use the following lines:\n",
    "\n",
    "#    stop_words = ['', 'in', 'my', 'may', 's', 'was', 'his', 'not', 'is', 'on', 'a', 'ten', 'for', 'be', 'an']\n",
    "#    bag        = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Some words that have different meanings are nevertheless refering to the same dimension of\n",
    "# reality. You could experiment with encoding this previous knowledge by merging such words:\n",
    "\n",
    "#    bag        = ['hi/lo' if word in {'high', 'low'} else word for word in bag]\n",
    "    \n",
    "    bags.append(bag)\n",
    "\n",
    "print0d('Words per document')\n",
    "print2d('{:12s}', bags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Combined vocabulary of all documents ===\n",
      "investment  deutsche  bank  yields  return  nothing  federer  good  volley  tennis  boring  year  high  \n"
     ]
    }
   ],
   "source": [
    "vocabulary = dict.fromkeys([word for bag in bags for word in bag])\n",
    "\n",
    "# We create the vocabulary from the documents by substracting\n",
    "# You could try to use a reduced vocabulary and see how LDA performs then:\n",
    "\n",
    "# vocabulary = dict.fromkeys(['investment', 'return', 'federer', 'volley'])\n",
    "# for bag in bags: bag = [word for word in bag if word in vocabulary]\n",
    "\n",
    "words = [word for word in vocabulary.keys()]\n",
    "\n",
    "print0d('Combined vocabulary of all documents')\n",
    "print1d('{}  ', words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Word counts in the documents ===\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "investment deutsche     bank   yields   return  nothing  federer     good   volley   tennis   boring     year     high\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "        1        1        1        1        1                                                                        \n",
      "        1                                   1        1                                                               \n",
      "                                            1                 1        1        1                                    \n",
      "                                            2                                   2        1        1                  \n",
      "        1                                   1                                                              1        1\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "        3        1        1        1        6        1        1        1        3        1        1        1        1\n"
     ]
    }
   ],
   "source": [
    "for key in vocabulary.keys(): vocabulary[key] = 0\n",
    "word_counts = np.zeros((len(bags), len(words)), dtype=int)\n",
    "\n",
    "for d, bag in enumerate(bags):\n",
    "    for w, word in enumerate(words):\n",
    "        \n",
    "        count = bag.count(word)\n",
    "        \n",
    "        vocabulary[word] += count\n",
    "        word_counts[d, w] = count\n",
    "\n",
    "LINE = (len(vocabulary) * 9 + 5) * '~'\n",
    "print0d('Word counts in the documents'); print(LINE)\n",
    "print1d('{:>9}', vocabulary.keys()    ); print(LINE)\n",
    "print2d('{:9d}', word_counts,        0); print(LINE)\n",
    "print1d('{:9d}', vocabulary.values()  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Words in the topics ===\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "investment deutsche     bank   yields   return  nothing  federer     good   volley   tennis   boring     year     high\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "      3.5      1.5      1.5      1.5      3.4      1.5      0.5      0.5      0.5      0.5      0.5      1.5      1.5\n",
      "      0.5      0.5      0.5      0.5      3.6      0.5      1.5      1.5      3.5      1.5      1.5      0.5      0.5\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    19.0%     8.1%     8.1%     8.1%    18.6%     8.1%     2.7%     2.7%     2.7%     2.7%     2.7%     8.1%     8.1%\n",
      "     3.1%     3.0%     3.0%     3.0%    21.6%     3.1%     9.0%     9.0%    21.0%     9.0%     9.0%     3.1%     3.1%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components = n_topics, learning_method='batch', max_iter=50, n_jobs = -1)\n",
    "\n",
    "lda.fit(word_counts)\n",
    "\n",
    "words_in_topics = normalize(lda.components_, norm='l1')\n",
    "\n",
    "print0d('Words in the topics'); print(LINE)\n",
    "print1d('{:>9}',   vocabulary.keys()); print(LINE)\n",
    "print2d('{:9.1f}', lda.components_  ); print(LINE)\n",
    "print2d('{:9.1%}', words_in_topics  ); print(LINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Topics in the documents ===\n",
      "Topic 0  Topic 1  \n",
      "    91%           \n",
      "    85%           \n",
      "             89%  \n",
      "             92%  \n",
      "    89%           \n"
     ]
    }
   ],
   "source": [
    "topics_in_corpus = lda.transform(word_counts)\n",
    "\n",
    "print0d('Topics in the documents')\n",
    "print1d('Topic{:2d}  ', range(n_topics)      )\n",
    "print2d('{:7.0%}  ',    topics_in_corpus, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\(^_^)/ The algorithm found the topic distribution that we expected. Nice!\n"
     ]
    }
   ],
   "source": [
    "actual_topics = np.argmax(topics_in_corpus, axis = 1)\n",
    "topics_are_as_expected = any([\n",
    "    np.array_equal(actual_topics, np.array(permutation)[expected_topics]) \n",
    "        for permutation in itertools.permutations(range(n_topics))\n",
    "])\n",
    "print('\\\\(^_^)/ The algorithm found the topic distribution that we expected. Nice!' if topics_are_as_expected \n",
    "    else '<(>_<)> NOOO!!! On this run the algorithm did NOT see the same topic distribution in the corpus as humans do.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Actual words in documents compared to estimated words in documents ===\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "investment deutsche     bank   yields   return  nothing  federer     good   volley   tennis   boring     year     high\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "        1        1        1        1        1                                                                        \n",
      "        1                                   1        1                                                               \n",
      "                                            1                 1        1        1                                    \n",
      "                                            2                                   2        1        1                  \n",
      "        1                                   1                                                              1        1\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "      0.9      0.4      0.4      0.4      0.9      0.4                                                   0.4      0.4\n",
      "      0.5                                 0.6                                                                        \n",
      "                                          0.9                                 0.8                                    \n",
      "                                          1.3               0.5      0.5      1.2      0.5      0.5                  \n",
      "      0.7                                 0.8                                                                        \n"
     ]
    }
   ],
   "source": [
    "length_in_corpus = [len(bag) for bag in bags]\n",
    "estimated_word_counts = np.diag(length_in_corpus).dot(topics_in_corpus).dot(words_in_topics)\n",
    "\n",
    "print0d('Actual words in documents compared to estimated words in documents'); print(LINE)\n",
    "print1d('{:>9}',   words                       ); print(LINE)\n",
    "print2d('{:9d}',   word_counts,           0    ); print(LINE)\n",
    "print2d('{:9.1f}', estimated_word_counts, 0.334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Short descriptions for the topics ===\n",
      "investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n"
     ]
    }
   ],
   "source": [
    "def topic_description(words, probabilities):\n",
    "\n",
    "    cumulated = 0\n",
    "    description = ''\n",
    "    \n",
    "    for w in np.argsort(probabilities)[::-1]:\n",
    "\n",
    "        probability = probabilities[w]\n",
    "        description += words[w]  + ','\n",
    "        \n",
    "        if (cumulated < 1/3 <= cumulated + probability) or (cumulated < 4/5 <= cumulated + probability):\n",
    "            description += '  '\n",
    "        \n",
    "        cumulated += probability\n",
    "    \n",
    "    return description.rstrip(' ').rstrip(',')\n",
    "\n",
    "descriptions = [topic_description(words, probabilities) for probabilities in words_in_topics]\n",
    "\n",
    "print0d('Short descriptions for the topics')\n",
    "print2d('{}', descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Investment in Deutsche Bank yields low return.\"\n",
      "X  91% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "-   9% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"My investment may return nothing.\"\n",
      "X  85% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "-  15% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"Federer’s return was good, his volley was not.\"\n",
      "-  11% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "X  89% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"Return volley, return volley; tennis is boring.\"\n",
      "-   8% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "X  92% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"Return on investment is on a ten year high.\"\n",
      "X  89% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "-  11% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n"
     ]
    }
   ],
   "source": [
    "for document, probabilities in zip(corpus, topics_in_corpus):\n",
    "\n",
    "    print('\\n\"{}\"'.format(document))\n",
    "    \n",
    "    for probability, description in zip(probabilities, descriptions):\n",
    "        print('{}  {:3.0%} {:}'.format('X' if probability > 0.5 else '-', probability, description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lattice of the \"Sentence uses word\" relation\n",
    "\n",
    "Have you been suspicious of whether we actually need a probabilistic approach to distinguish these few documents? If yes, you were right. The lattice below illustrates, which document contains which word. A document contains a word if you can reach a word starting from the document by following lines upwards. As you see, we could just ignore \"return\" as all documents contain this word. The presence of \"investment\" or \"volley\" separates the corpus into two. The remaining words are then just specific to each of the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/lda-on-returns-word-use-in-5-sentences.PNG' style='width:60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To distinguish the topics words that appear in (almost) all documents are as uninteresting as words that appear only in (not much more than) one document. For a larger corpus you might thus filter words that are in many documents (we had good results with an upper bound of 50%, but even lower values might work) as well as very few documents (we had reasonable good results with a lower bound of 3 documents given a corpus of about 1000 documents, but even higher values might work)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lattice was created with \"The Concept Explorer\" version 1.3, created by Serhiy A. Yevtushenko, available at http://conexp.sourceforge.net/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lattice of the \"Sentence uses word\" relation, given two more sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it is time to increase the corpus a bit. Scroll back to the top and include the given two more sentences. The lattice below demonstrates that an analysis based just on set theory becomes harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/lda-on-returns-word-use-in-7-sentences.PNG' style='width:60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is now a sentence that does not include \"return\" and while \"investment\" would still be enough to identify the one topic \"volley\" is not part of all sentences belonging to the other topic (\"tennis\"). Each of the three sentences of the topic \"tennis\" contains two of the three words \"tennis\", \"federer\", \"volley\". So, one could say that the probability of a document containing one of these words given the document belongs to the topic \"tennis\" is 2/3.\n",
    "\n",
    "The actual probabilistic model behind LDA is slightly different. It assumes that the whole corpus is generated as follows: The topic shares are distributed according to a Dirichlet distribution (https://en.wikipedia.org/wiki/Dirichlet_distribution). Within each topic each word in the vocabulary has a certain probability (https://en.wikipedia.org/wiki/Multinomial_distribution). For each document first the topic shares are randomly picked. Then for each word in the document first a topic is randomly picked according to the topic shares and then the actual word is randomly picked according to the probabilities of the words in the topic. A good overview can be found in: \n",
    "Blei, D. M. (2012). Probabilistic Topic Models. Communications of the ACM, 55(4), 77–84 (https://doi.org/10.1145/2133806.2133826)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the learned topic model to new sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had trained the probabilities of the words within the topics. These can be used to estimate the topic shares within new sentences that were not used for training. Here are a few. Obviously they were created to challenge the model. See how the model performs. You might want to compare the results for a model fitted to the corpus of 5 and for the corpus of 7. The two more sentences seem to make quite a difference. Try for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"I want to return from tennis.\"\n",
      "-  20% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "X  80% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"Investment is nothing boring.\"\n",
      "X  66% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "-  34% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"Tennis is nothing boring.\"\n",
      "-  32% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "X  68% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"After a good return, I return to volley.\"\n",
      "-  12% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "X  88% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"This is my year of high investment.\"\n",
      "X  87% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "-  13% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"Federer's investment is at Deutsche Bank\"\n",
      "X  87% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "-  13% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"Playing tennis for hours is quite an investment.\"\n",
      "X  53% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "-  47% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\n",
      "=== Actual words in documents compared to estimated words in documents ===\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "investment deutsche     bank   yields   return  nothing  federer     good   volley   tennis   boring     year     high\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "                                            1                                            1                           \n",
      "        1                                            1                                            1                  \n",
      "                                                     1                                   1        1                  \n",
      "                                            2                          1        1                                    \n",
      "        1                                                                                                  1        1\n",
      "        1        1        1                                                                                          \n",
      "        1                                                                                1                           \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "                                          0.8                                 0.7                                    \n",
      "      0.4                                 0.6                                                                        \n",
      "                                          0.6                                 0.5                                    \n",
      "                                          1.1               0.4      0.4      0.9      0.4      0.4                  \n",
      "      0.7                                 0.8                                                                        \n",
      "      0.7                                 0.8                                                                        \n",
      "      0.6                                 1.0                                 0.6                                    \n"
     ]
    }
   ],
   "source": [
    "more_documents = [\n",
    "    'I want to return from tennis.',\n",
    "    'Investment is nothing boring.',\n",
    "    'Tennis is nothing boring.',\n",
    "    'After a good return, I return to volley.',\n",
    "    'This is my year of high investment.',\n",
    "    'Federer\\'s investment is at Deutsche Bank',\n",
    "    'Playing tennis for hours is quite an investment.',\n",
    "]\n",
    "\n",
    "more_bags = [[token.lower() for token in re.split('[ .!,;’]', document) if len(token) > 3] for document in more_documents]\n",
    "more_word_counts = [[bag.count(word) for word in words] for bag in more_bags]\n",
    "\n",
    "topics_in_more_documents = lda.transform(more_word_counts)\n",
    "\n",
    "for document, probabilities in zip(more_documents, topics_in_more_documents):\n",
    "    print('\\n\"{}\"'.format(document))\n",
    "    for probability, description in zip(probabilities, descriptions):\n",
    "        print('{} {:3.0%} {:}'.format('X ' if probability > 0.5 else '- ', probability, description))     \n",
    "\n",
    "estimated_word_counts = np.diag([len(bag) for bag in more_bags]).dot(topics_in_more_documents).dot(words_in_topics)\n",
    "\n",
    "print()\n",
    "print0d('Actual words in documents compared to estimated words in documents'); print(LINE)\n",
    "print1d('{:>9}',   words                       ); print(LINE)\n",
    "print2d('{:9d}',   more_word_counts,      0    ); print(LINE)\n",
    "print2d('{:9.1f}', estimated_word_counts, 0.334)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go from here\n",
    "\n",
    "Have you played around enough with this notebook already? You should have tried some of the suggested variations. Here are some ideas about where to continue from here. They are in no particular order. Choose depending on your interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the maths\n",
    "\n",
    "With a good background in probability theory you might study the original paper: Blei, D. M., Ng, A. Y., Jordan, M. I. (2003). Latent Dirichlet Allocation. Journal of Machine Learning Research, 3, 993–1022. (https://dl.acm.org/citation.cfm?id=944919.944937)\n",
    "\n",
    "The following lecture about Pattern Recognition by Prof. Christian Bauckhage might be helpful. While none of the lectures  addresses Latent Dirichlet Allocation directly, they provide important background knowledge.\n",
    "  * [Pattern Recognition, Lecture 09, Maximum Likelihood Techniques](https://www.researchgate.net/project/lectures-on-pattern-recognition/update/5cd1b0f33843b0b98251ae33)\n",
    "  * [Pattern Recognition, Lecture 10, Bayesian inference](https://www.researchgate.net/project/lectures-on-pattern-recognition/update/5cdfd7653843b0b982536a37)\n",
    "  * [Pattern Recognition, Lecture 14, Expectation Maximization](https://www.researchgate.net/project/lectures-on-pattern-recognition/update/5cee3bd63843b0b98254cfd9), Expectation Maximization is the algorithm behind Latent Dirichlet Allocation in the original paper. The EM-Algorithm is covered in this lecture. But! The LDA covered in this lecture is Fishers Latent Discriminant Analysis. \n",
    "\n",
    "We have prepared two notebooks illustrating the EM-Algorithm for Gaussian Mixture Models:\n",
    "  * [Expectation Maximization for Gaussian Mixture Models](https://nbviewer.jupyter.org/github/p3ml/recipes/blob/master/Expectation%20Maximization%20for%20Gaussian%20Mixture%20Models.ipynb)\n",
    "  * [EM-Algorithm for GMMs - Detailed Visualization and Playground](https://nbviewer.jupyter.org/github/p3ml/recipes/blob/master/EM-Algorithm%20for%20GMMs%20-%20Detailed%20Visualization%20and%20Playground.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work on a larger corpus\n",
    "\n",
    "Why let a machine do something that you can do better? You should have a look at some data where it is helpful that machines are faster than us or that they can process larger amounts of data than a human. We prepared an experiment here:\n",
    "\n",
    "https://p3ml.github.io/#notebooks-about-latent-dirichlet-allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with other implementations\n",
    "\n",
    "As the original paper about Latent Dirichlet Allocation was already published in 2003, it comes at no surprise that there is already a lot of useful material available. Here are two implementations and an exciting visualization:\n",
    "\n",
    "  * scikit-learn's LatentDirichletAllocation: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html used above, you may leave the counting to https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html.\n",
    "  * Radim Řehůřek's gensim: https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "  * pyLDAvis - Python library for interactive topic model visualization: https://github.com/bmabey/pyLDAvis\n",
    "\n",
    "A ready made suite to apply LDA on a corpus and to navigate the corpus based on the topic model is available here:\n",
    "\n",
    "  * InPhO Topic Explorer: https://www.hypershelf.org/\n",
    "  * See for example the model created for the \"Stanford Encyclopedia of Philosophy (Spring 2018)\" with 80 topics focusing on the document titled \"Turing Machine\": https://www.hypershelf.org/sep/80/?doc=turing-machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactor this notebook and compare the options side by side\n",
    "If you have some experience in software development, you might not be satisfied with having to change notebook and to re-execute the whole notebook to compare results. You might want to wrap the steps into functions with parameters instead of working on global variables in top level statements. With a bit preparation it should be possible to compare the performance of the model that were fitted to different corpora after different pre-processing side by side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "      <td colspan=\"1\" style=\"text-align:left;background-color:#0071BD;color:white\">\n",
    "        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">\n",
    "            <img alt=\"Creative Commons License\" style=\"border-width:0;float:left;padding-right:10pt\"\n",
    "                 src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" />\n",
    "        </a>\n",
    "        &copy; D. Speicher<br/>\n",
    "        Licensed under a \n",
    "        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\" style=\"color:white\">\n",
    "            CC BY-NC 4.0\n",
    "        </a>.\n",
    "      </td>\n",
    "      <td colspan=\"2\" style=\"text-align:left;background-color:#66A5D1\">\n",
    "          <b>Acknowledgments:</b>\n",
    "          This material was prepared within the project\n",
    "          <a href=\"http://www.b-it-center.de/b-it-programmes/teaching-material/p3ml/\" style=\"color:black\">\n",
    "              P3ML\n",
    "          </a> \n",
    "          which is funded by the Ministry of Education and Research of Germany (BMBF)\n",
    "          under grant number 01/S17064. The authors gratefully acknowledge this support.\n",
    "      </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
